"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.writeStreamToBytes = exports.readStreamFromBytes = exports.compress = exports.decompress = void 0;
// This import has to come first so that the polyfills are registered before the stream polyfills
const proto_1 = require("@xmtp/proto");
//
// Compression
//
async function decompress(encoded, maxSize) {
    if (encoded.compression === undefined) {
        return;
    }
    const sink = { bytes: new Uint8Array(encoded.content.length) };
    await readStreamFromBytes(encoded.content)
        .pipeThrough(new DecompressionStream(compressionIdFromCode(encoded.compression)))
        .pipeTo(writeStreamToBytes(sink, maxSize));
    encoded.content = sink.bytes;
}
exports.decompress = decompress;
async function compress(encoded) {
    if (encoded.compression === undefined) {
        return;
    }
    const sink = { bytes: new Uint8Array(encoded.content.length / 10) };
    await readStreamFromBytes(encoded.content)
        .pipeThrough(new CompressionStream(compressionIdFromCode(encoded.compression)))
        .pipeTo(writeStreamToBytes(sink, encoded.content.length + 1000));
    encoded.content = sink.bytes;
}
exports.compress = compress;
function compressionIdFromCode(code) {
    if (code === proto_1.content.Compression.COMPRESSION_GZIP) {
        return 'gzip';
    }
    if (code === proto_1.content.Compression.COMPRESSION_DEFLATE) {
        return 'deflate';
    }
    throw new Error('unrecognized compression algorithm');
}
function readStreamFromBytes(bytes, chunkSize = 1024) {
    let position = 0;
    return new ReadableStream({
        pull(controller) {
            if (position >= bytes.length) {
                return controller.close();
            }
            let end = position + chunkSize;
            end = end <= bytes.length ? end : bytes.length;
            controller.enqueue(bytes.subarray(position, end));
            position = end;
        },
    });
}
exports.readStreamFromBytes = readStreamFromBytes;
function writeStreamToBytes(sink, maxSize) {
    let position = 0;
    return new WritableStream({
        write(chunk) {
            const end = position + chunk.length;
            if (end > maxSize) {
                throw new Error('maximum output size exceeded');
            }
            while (sink.bytes.length < end) {
                sink.bytes = growBytes(sink.bytes, maxSize);
            }
            sink.bytes.set(chunk, position);
            position = end;
        },
        close() {
            if (position < sink.bytes.length) {
                sink.bytes = sink.bytes.subarray(0, position);
            }
        },
    });
}
exports.writeStreamToBytes = writeStreamToBytes;
function growBytes(bytes, maxSize) {
    let newSize = bytes.length * 2;
    if (newSize > maxSize) {
        newSize = maxSize;
    }
    const bigger = new Uint8Array(newSize);
    bigger.set(bytes);
    return bigger;
}
//# sourceMappingURL=Compression.js.map